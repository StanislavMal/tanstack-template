// üìÑ src/lib/ai/providers/gemini-openai.ts

import OpenAI from 'openai';
import type { AIProvider, Message, AIModel, AIProviderConfig, StreamChunk } from '../types';

export class GeminiOpenAIProvider implements AIProvider {
  name = 'Gemini (OpenAI Compatible)';
  
  private apiKeys: string[];
  private currentKeyIndex = 0;

  constructor() {
    // –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏
    this.apiKeys = Object.keys(process.env)
      .filter(key => key.startsWith('GEMINI_API_KEY_') && process.env[key])
      .map(key => process.env[key] as string);
    
    if (this.apiKeys.length === 0) {
      throw new Error('No GEMINI_API_KEY_* environment variables found');
    }
  }

  private getNextApiKey(): string {
    const key = this.apiKeys[this.currentKeyIndex];
    this.currentKeyIndex = (this.currentKeyIndex + 1) % this.apiKeys.length;
    return key;
  }

  async streamChat(messages: Message[], config: Partial<AIProviderConfig>): Promise<ReadableStream<Uint8Array>> {
    const apiKey = this.getNextApiKey();
    
    const openai = new OpenAI({
      apiKey,
      baseURL: 'https://generativelanguage.googleapis.com/v1beta/openai/',
    });

    // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç OpenAI
    const openAIMessages = messages.map(msg => ({
      role: msg.role as 'user' | 'assistant' | 'system',
      content: msg.content,
    }));

    // –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
    if (config.systemInstruction) {
      openAIMessages.unshift({
        role: 'system',
        content: config.systemInstruction,
      });
    }

    // –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    const requestOptions: any = {
      model: config.model || 'gemini-2.5-flash',
      messages: openAIMessages,
      stream: true,
      temperature: config.temperature || 0.7,
      max_tokens: config.maxTokens || 8192,
    };

    // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–µ–π 2.5
    if (config.model?.includes('2.5')) {
      const isProModel = config.model === 'gemini-2.5-pro';
      
      // –î–ª—è Pro –º–æ–¥–µ–ª–µ–π reasoning –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω
      if (isProModel) {
        requestOptions.reasoning_effort = config.reasoningEffort || 'low';
      } else if (config.reasoningEffort && config.reasoningEffort !== 'none') {
        // –î–ª—è Flash –º–æ–¥–µ–ª–µ–π reasoning –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω
        requestOptions.reasoning_effort = config.reasoningEffort;
      }
      
      // –í–∫–ª—é—á–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º—ã—Å–ª–µ–π —á–µ—Ä–µ–∑ extra_body –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å reasoning
      if (requestOptions.reasoning_effort && requestOptions.reasoning_effort !== 'none') {
        requestOptions.extra_body = {
          google: {
            thinking_config: {
              include_thoughts: true
            }
          }
        };
      }
    }

    try {
      console.log(`Using Gemini model: ${requestOptions.model}, reasoning: ${requestOptions.reasoning_effort || 'none'}`);
      
      // –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
      console.log('Request options:', {
        model: requestOptions.model,
        reasoning_effort: requestOptions.reasoning_effort,
        has_extra_body: !!requestOptions.extra_body,
        message_count: requestOptions.messages.length
      });

      const response = await openai.chat.completions.create(requestOptions);

      // –°–æ–∑–¥–∞–µ–º ReadableStream –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
      return new ReadableStream({
        async start(controller) {
          const encoder = new TextEncoder();
          let reasoningBuffer = '';
          
          try {
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∏–º—É
            if (Symbol.asyncIterator in response) {
              for await (const chunk of response as AsyncIterable<any>) {
                const content = chunk.choices[0]?.delta?.content;
                
                // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º reasoning content –µ—Å–ª–∏ –µ—Å—Ç—å
                if (chunk.choices[0]?.delta?.reasoning_content) {
                  reasoningBuffer += chunk.choices[0]?.delta?.reasoning_content;
                  const reasoningChunk: StreamChunk = { 
                    reasoning: chunk.choices[0]?.delta?.reasoning_content 
                  };
                  controller.enqueue(encoder.encode(JSON.stringify(reasoningChunk) + '\n'));
                }
                
                if (content) {
                  const streamChunk: StreamChunk = { text: content };
                  // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
                  controller.enqueue(encoder.encode(JSON.stringify(streamChunk) + '\n'));
                }
              }
            } else {
              // –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π OpenAI
              const stream = response as any;
              for await (const chunk of stream) {
                const content = chunk.choices[0]?.delta?.content;
                
                // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º reasoning content –µ—Å–ª–∏ –µ—Å—Ç—å
                if (chunk.choices[0]?.delta?.reasoning_content) {
                  reasoningBuffer += chunk.choices[0]?.delta?.reasoning_content;
                  const reasoningChunk: StreamChunk = { 
                    reasoning: chunk.choices[0]?.delta?.reasoning_content 
                  };
                  controller.enqueue(encoder.encode(JSON.stringify(reasoningChunk) + '\n'));
                }
                
                if (content) {
                  const streamChunk: StreamChunk = { text: content };
                  controller.enqueue(encoder.encode(JSON.stringify(streamChunk) + '\n'));
                }
              }
            }
            
            const finalChunk: StreamChunk = { 
              finished: true,
              reasoning: reasoningBuffer || undefined
            };
            controller.enqueue(encoder.encode(JSON.stringify(finalChunk) + '\n'));
          } catch (error) {
            console.error('Error in stream processing:', error);
            const errorChunk: StreamChunk = { 
              error: error instanceof Error ? error.message : 'Unknown error occurred' 
            };
            controller.enqueue(encoder.encode(JSON.stringify(errorChunk) + '\n'));
          } finally {
            controller.close();
          }
        },
      });
    } catch (error: any) {
      console.error('Error in GeminiOpenAIProvider:', error);
      
      // –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
      let errorMessage = 'Unknown Gemini API error';
      
      if (error.status === 400) {
        errorMessage = 'Bad request - check model name and parameters';
      } else if (error.status === 401) {
        errorMessage = 'Invalid API key';
      } else if (error.status === 403) {
        errorMessage = 'API key does not have access to this model';
      } else if (error.status === 429) {
        errorMessage = 'Rate limit exceeded';
      } else if (error.status === 500) {
        errorMessage = 'Internal server error';
      } else if (error.message) {
        errorMessage = error.message;
      }
      
      throw new Error(`Gemini API Error: ${errorMessage} (status: ${error.status})`);
    }
  }

  getAvailableModels(): AIModel[] {
    return [
      {
        id: 'gemini-2.5-flash',
        name: 'Gemini 2.5 Flash',
        provider: 'gemini',
        description: 'Latest Flash model with reasoning capabilities',
        contextWindow: 1000000,
        maxOutputTokens: 8192,
        supportsFunctions: true,
        supportsVision: true,
        supportsAudio: true,
        reasoning: {
          supported: true,
          levels: ['low', 'medium', 'high'],
        },
      },
      {
        id: 'gemini-2.5-pro',
        name: 'Gemini 2.5 Pro',
        provider: 'gemini',
        description: 'Most capable model with advanced reasoning',
        contextWindow: 2000000,
        maxOutputTokens: 8192,
        supportsFunctions: true,
        supportsVision: true,
        supportsAudio: true,
        reasoning: {
          supported: true,
          levels: ['low', 'medium', 'high'],
        },
      },
    ];
  }
}